{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But du Projet: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce travail est d'analyser et de classifier un échantillon de données , qui sont des document colléctés et numérisé, et qui peuvent être désignés comme preuve de l'existance d'un lien entre la consommation du Tabac et des maladies graves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'ensemble de données  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données qu'on a à notre possession ont été classés dans des répertoires correspondants aux classes de documents:\n",
    "\n",
    "- Advertisement\n",
    "- Email\n",
    "- Form\n",
    "- Letter\n",
    "- Memo\n",
    "- News\n",
    "- Note\n",
    "- Report\n",
    "- Resume\n",
    "- Scientific\n",
    " \n",
    "Chaque classe contient plusieurs documents .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des données: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celon le graphe ci dessous, on remarque des classes dominantes par rapport aux autres, comme les classes Email, Letter, Memo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré traitement des données:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape pourrait être primordiale , car c'est une approche qui  consiste à supposer que la plus petite unité d'information dans un texte est le mot . Nous allons donc représenter nos textes sous forme de séquences de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela, on va proceder comme suit:\n",
    "        * Supprimer les tags html(s'ils existents)\n",
    "        * Remplacer les ponctuations par des espaces\n",
    "        * Remplacer les lettres majuscule en minuscule . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation des données : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En géneral on a besoin de 2 sets de données ( Training , Testing) \n",
    "*   **Training set** :ou le jeu de données d'apprentissage , est le jeu de données initial utilisé pour former un algorithme afin de comprendre comment appliquer des technologies telles que les réseaux de neurones, pour apprendre et produire des résultats complexes. Il inclut les données d'entrée et la sortie attendue correspondante. Le but du jeu de données d'apprentissage est de fournir à votre algorithme des données de «vérité sur le terrain».\n",
    "*  **Testing set** : ou le jeu de données de test, cependant, est utilisé pour évaluer le degré de l'apprentissage de votre algorithme . Vous ne pouvez pas simplement réutiliser le jeu de données d'apprentissage lors de la phase de test car l'algorithme \"connaît\" déjà la sortie attendue, ce qui va à l'encontre de l'objectif de test de l'algorithme.\n",
    " \n",
    "On peut ajouter un 3ème set qui est le jeu de donnée de validation ,qui seert en géneral à fournir une évaluation non biaisée d'un ajustement de modèle sur l'ensemble de données d'apprentissage tout en ajustant les hyperparamètres du modèle (par exemple, le nombre d'unités cachées dans un réseau de neurones ). Les jeux de données de validation peuvent être utilisés pour la régularisation en arrêtant tôt: arrêtez la formation lorsque le nombre d'erreurs sur le jeu de données de validation augmente, car il s'agit d'un signe de surajustement pour le jeu de données de formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"7.png\"  width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Classification : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai utilisé trois classifieurs :\n",
    "1. Classifieur Bayesien Naif .\n",
    "2. Reseau de neurones Convolutionnel .\n",
    "3. Reseau de neuronnes Multi couches.\n",
    "\n",
    "L'objectif est de preserver le clssifieur avec le plus grand score , pour cela je vais utiliser 3 metriques d'evaluation (Precision, Recall ,F1 score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi je vais utiliser les matrices de confusion pour chaque classifieur , qui est , dans la terminologie de l'apprentissage supervisé, un outil servant à mesurer la qualité d'un système de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dans python , sklearn.metrics propose 2 librairies:classification_report,confusion_matrix.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Classifier Bayésien Naif "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Vectorisation **:Pour appliquer des algorithmes d'apprentissage automatique au texte, les documents doivent être transformés en vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai utilisé ce classifieur pour 3 representations differents:\n",
    "* Bag of Word\n",
    "* Word Level TF-IDF\n",
    "* N-gram Level TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representation Bag of Word **:\n",
    " Le moyen le plus simple et le plus classique de transformer un document en vecteur est l’encodage en sac de mots.\n",
    " 1. Définir l'ensemble de tous les mots possibles pouvant figurer dans un document; notez sa taille par max_features.\n",
    " 2. Pour chaque document, encodez-le avec un vecteur de taille max_features, avec la valeur de la ième composante du vecteur égale au nombre de fois où le ième mot apparaît dans le document.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word level TF-IDF **:\n",
    "    MAtrice representant les scores tf-idf de chaque terme dans les differents documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram Level TF-IDF **:N-grams est la combinaison de N termes . Et cette MAtrice represente les scores tf-idf de N_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les resultats du classifieur bayésien avec une representation (N_gram Level TF_IDF) donne de mailleur resultats que les autres representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.Reseau de Neurone Convolutionnel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general les reseaux de neurones Convolutionnel servent à capturer des informations hiérarchiques ,\n",
    "L’essentiel d’un CNN est de regarder une région de l’entrée à la fois, de l’affecter à une sortie et de répéter ce processus pour chaque région de l’entrée.\n",
    "En plaçant une série de convolutions l’une après l’autre, nous faisons apprendre notre réseau de manière hiérarchique: chaque couche suivante est une convolution des valeurs de la couche précédente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie j'ai utilisé un model simple , avec géneralement une couche CNN, une couche maxPlooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pooling sert à  :\n",
    "* Réduire la dimension de chaque feature map \n",
    "* Prendre l’information la plus importante.\n",
    "\n",
    "Et si on choisit le Max Pooling:\n",
    " * Prend le maximum d’un voisinage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ** DropOut **: est une technique ne regularisation , qui prend d’eune manière aléatoire quelque neurones qui va les ignorer pendant l’entrainement.au fur et a mesure qu'un réseau de neurones apprend, les poids de neurones s'installent dans leur contexte au sein du réseau.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.Reseau de neuronnes Multi Couches (Multi Layer Perceptron) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi Layer Perceptron**  (MLP) ,est une classe de réseaux neuronaux artificiels à anticipation. Un MLP se compose d'au moins trois couches de nœuds: une couche d'entrée, une couche cachée et une couche de sortie. À l'exception des nœuds d'entrée, chaque nœud est un neurone utilisant une fonction d'activation non linéaire. MLP utilise une technique d'apprentissage supervisée appelée rétropropagation pour la formation. Ses multiples couches et son activation non linéaire distinguent MLP d'un perceptron linéaire. Il peut distinguer des données qui ne sont pas séparables linéairement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
